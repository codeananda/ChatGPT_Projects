{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models and Prompt Templates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adapted from Lesson 2 of [LangChain for LLM Application Development](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/) by deeplearning.ai."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Set up model\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "# Set up prompt template\n",
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True), additional_kwargs={})"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why is it messages plural?\n",
    "prompt_template.messages[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['style', 'text']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to pass these each time we use the template - much nicer and more modular!\n",
    "prompt_template.messages[0].prompt.input_variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[HumanMessage(content=\"Translate the text that is delimited by triple backticks into a style that is British English in a calm, friendly, and respectful tone. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\", additional_kwargs={}, example=False)]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_style = \"\"\"British English in a calm, friendly, and respectful tone\"\"\"\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt using the template\n",
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=output_style,\n",
    "                    text=customer_email)\n",
    "\n",
    "customer_messages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Arrr, I must say I'm quite frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to add to my troubles, the warranty doesn't cover the cost of cleaning up my kitchen. I would greatly appreciate your assistance at this moment, my friend!\""
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_response = chat(customer_messages)\n",
    "# Now translated into British English\n",
    "customer_response.content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\\n```\\n\""
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's translate the service reply back into the original language (Pirate English)\n",
    "\n",
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\"\n",
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\"\n",
    "\n",
    "# This name is a bit odd. We aren't formatting messages, we are formatting the prompt\n",
    "service_messages = prompt_template.format_messages(\n",
    "                    style=service_style_pirate,\n",
    "                    text=service_reply)\n",
    "\n",
    "service_messages[0].content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Ahoy there, matey! I regret to inform ye that the warranty be not coverin' the costs o' cleanin' yer galley, as 'tis yer own fault fer misusin' yer blender by forgettin' to secure the lid afore startin' it. Aye, tough luck, me heartie! Fare thee well!\""
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the service response (originally in English) is in the user's language (Pirate English)\n",
    "service_response = chat(service_messages)\n",
    "service_response.content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Output Parsers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Slow, Brittle Way"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gift\": false,\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "# Define our own template\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n",
    "# Create prompt template from our template string\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "# Fill in prompt template to create messages\n",
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "# Pass messages to chat and get response\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "response = chat(messages)\n",
    "# This is brittle because the response is a string and still requires further processing\n",
    "print(response.content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Output Parsers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Define each part of the response as a schema\n",
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema,\n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]\n",
    "\n",
    "# Create output parser\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "# Automatically (!) create format instructions for the model output\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original response content:  ```json\n",
      "{\n",
      "\t\"gift\": false,\n",
      "\t\"delivery_days\": \"2\",\n",
      "\t\"price_value\": \"slightly more expensive than the other leaf blowers out there\"\n",
      "}\n",
      "```\n",
      "Parsed response:  {'gift': False, 'delivery_days': '2', 'price_value': 'slightly more expensive than the other leaf blowers out there'}\n",
      "\n",
      "!!! Parsed response is type:  <class 'dict'>  !!!\n",
      "\n",
      "parsed_response.get(\"gift\"):  False\n"
     ]
    }
   ],
   "source": [
    "# This seems a bit excessive length wise\n",
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "# Much shorter and gets the job done.\n",
    "review_template_3 = \"\"\"\n",
    "For the following text, extract the following information and format it like this:\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_3)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review,\n",
    "                                format_instructions=format_instructions)\n",
    "response = chat(messages)\n",
    "print('Original response content: ', response.content)\n",
    "parsed_response = output_parser.parse(response.content)\n",
    "print('Parsed response: ', parsed_response)\n",
    "print()\n",
    "print('!!! Parsed response is type: ', type(parsed_response), ' !!!')\n",
    "print()\n",
    "print('parsed_response.get(\"gift\"): ', parsed_response.get(\"gift\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationTokenBufferMemory,\n",
    "    ConversationSummaryBufferMemory\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# Model + memory = conversation\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=True # shows full prompt for each prediction\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello, it's really sunny today!\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"Hello! Yes, it is indeed sunny today. The temperature is around 25 degrees Celsius, and there are only a few scattered clouds in the sky. It's a perfect day to go outside and enjoy the sunshine!\""
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hello, it's really sunny today!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, it's really sunny today!\n",
      "AI: Hello! Yes, it is indeed sunny today. The temperature is around 25 degrees Celsius, and there are only a few scattered clouds in the sky. It's a perfect day to go outside and enjoy the sunshine!\n",
      "Human: Is it so hot that I need to wear suncream?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"The temperature is 25 degrees Celsius, which is not extremely hot. However, it's always a good idea to wear sunscreen when you're going to be exposed to the sun for an extended period of time, regardless of the temperature. Sunscreen helps protect your skin from harmful UV rays, which can cause sunburn and increase the risk of skin cancer. So, it's better to be safe and apply some sunscreen before heading out.\""
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Is it so hot that I need to wear suncream?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, it's really sunny today!\n",
      "AI: Hello! Yes, it is indeed sunny today. The temperature is around 25 degrees Celsius, and there are only a few scattered clouds in the sky. It's a perfect day to go outside and enjoy the sunshine!\n",
      "Human: Is it so hot that I need to wear suncream?\n",
      "AI: The temperature is 25 degrees Celsius, which is not extremely hot. However, it's always a good idea to wear sunscreen when you're going to be exposed to the sun for an extended period of time, regardless of the temperature. Sunscreen helps protect your skin from harmful UV rays, which can cause sunburn and increase the risk of skin cancer. So, it's better to be safe and apply some sunscreen before heading out.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "{'history': \"Human: Hello, it's really sunny today!\\nAI: Hello! Yes, it is indeed sunny today. The temperature is around 25 degrees Celsius, and there are only a few scattered clouds in the sky. It's a perfect day to go outside and enjoy the sunshine!\\nHuman: Is it so hot that I need to wear suncream?\\nAI: The temperature is 25 degrees Celsius, which is not extremely hot. However, it's always a good idea to wear sunscreen when you're going to be exposed to the sun for an extended period of time, regardless of the temperature. Sunscreen helps protect your skin from harmful UV rays, which can cause sunburn and increase the risk of skin cancer. So, it's better to be safe and apply some sunscreen before heading out.\"}"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "'Human: Yo\\nAI: Sup dawg'"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({'input': 'Yo'}, {'output': 'Sup dawg'})\n",
    "memory.buffer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "{'history': 'Human: Yo\\nAI: Sup dawg'}"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "'Human: Yo\\nAI: Sup dawg\\nHuman: Nice trainers\\nAI: Thanks bro, they were expensive'"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({'input': 'Nice trainers'}, {'output': 'Thanks bro, they were expensive'})\n",
    "memory.buffer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ConversationBufferWindowMemory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Oh, pizza! That's a popular choice for many people. There are so many different types of pizza toppings and crusts to choose from. Do you have a specific favorite type of pizza?\""
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just remember the last message\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "conversation.predict(input='my favourite food is pizza')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "'The capital of Germany is Berlin.'"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='what is the capital of Germany?')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "\"I'm sorry, but I don't have access to personal information about individuals, so I don't know what your favorite food is.\""
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doesn't remember as we set k=1\n",
    "conversation.predict(input='what is my favourite food?')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "{'history': \"Human: what is my favourite food?\\nAI: I'm sorry, but I don't have access to personal information about individuals, so I don't know what your favorite food is.\"}"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This loads the variables actually in memory that the model uses\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "# memory.buffer stores the whole convo and isn't necessarily what the model has access to at any given time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ConversationTokenBufferMemory\n",
    "\n",
    "Same thing but limit is on the number of tokens."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ConversationSummaryMemory\n",
    "\n",
    "This is really cool!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "{'history': 'System: The human and AI exchange greetings. The human mentions that they are eating breakfast. The AI informs the human about their schedule for the day, including a meeting with the product team, working on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.'}"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging and eating my breakfast\"},\n",
    "                    {\"output\": \"Coolio\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
    "                    {\"output\": f\"{schedule}\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI exchange greetings. The human mentions that they are eating breakfast. The AI informs the human about their schedule for the day, including a meeting with the product team, working on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"A good demo to show during the lunch meeting would be the latest version of the Language Learning Model (LLM) developed by our team. The LLM is an advanced AI model that can understand and generate human-like text in multiple languages. It has been trained on a vast amount of data and can perform tasks such as language translation, text summarization, and even creative writing. By showcasing the LLM's capabilities, we can demonstrate the power and potential of AI in various applications.\""
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "{'history': \"System: The human and AI exchange greetings. The human mentions that they are eating breakfast. The AI informs the human about their schedule for the day, including a meeting with the product team, working on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting. The human asks what would be a good demo to show, and the AI suggests showcasing the Language Learning Model (LLM), an advanced AI model that can understand and generate human-like text in multiple languages. The LLM has been trained on a vast amount of data and can perform tasks such as language translation, text summarization, and creative writing. By showcasing the LLM's capabilities, the AI believes they can demonstrate the power and potential of AI in various applications.\"}"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
