{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Based on the given text \"Hallo, ich bin 25 Jahre alt,\" it can be classified as CEFR level A1.'}, {'text': 'A2'}, {'text': 'A2'}]\n",
      "apply time taken: 2.87 seconds\n",
      "\n",
      "[{'text': 'Based on the given text \"Hallo, ich bin 25 Jahre alt,\" it can be classified as CEFR level A1.'}, {'text': 'A2'}, {'text': 'A2'}]\n",
      "aapply time taken: 1.34 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    'Classify the text based on the Common European Framework of Reference '\n",
    "    'for Languages (CEFR). Give a single value: {text}',\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "texts = [\n",
    "    {'text': 'Hallo, ich bin 25 Jahre alt.'},\n",
    "    {'text': 'Wie geht es dir?'},\n",
    "    {'text': 'In meiner Freizeit, spiele ich gerne Fussball.'}\n",
    "]\n",
    "\n",
    "start = time()\n",
    "res_a = chain.apply(texts)\n",
    "print(res_a)\n",
    "print(f\"apply time taken: {time() - start:.2f} seconds\")\n",
    "print()\n",
    "\n",
    "start = time()\n",
    "res_aa = await chain.aapply(texts)\n",
    "print(res_aa)\n",
    "print(f\"aapply time taken: {time() - start:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                   Product                                             Review\n0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n4  Milk Frother Handheld\\n   I loved this product. But they only seem to l...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Product</th>\n      <th>Review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Queen Size Sheet Set</td>\n      <td>I ordered a king size set. My only criticism w...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Waterproof Phone Pouch</td>\n      <td>I loved the waterproof sac, although the openi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Luxury Air Mattress</td>\n      <td>This mattress had a small hole in the top of i...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Pillows Insert</td>\n      <td>This is the best throw pillow fillers on Amazo...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Milk Frother Handheld\\n</td>\n      <td>I loved this product. But they only seem to l...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/product_reviews.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Langy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classify Level of Text and Provide Reason"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"level\": string  // The CEFR level of the text, e.g. A1, B2, etc.\n",
      "\t\"reason\": string  // The reason for the classification\n",
      "}\n",
      "```\n",
      "{'level': 'A2', 'reason': 'The text demonstrates a basic understanding of the language with simple sentence structures and limited vocabulary. The use of present tense and basic expressions indicate a beginner level of proficiency.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": "dict"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "cefr_template =\"\"\"Classify the text based on the Common European Framework of Reference\n",
    "for Languages (CEFR), provide a reason for your answer.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "{format_instructions}\n",
    " \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(cefr_template)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define output schema\n",
    "level_schema = ResponseSchema(name=\"level\",\n",
    "                             description=\"The CEFR level of the text, e.g. A1, B2, etc.\")\n",
    "reason_schema = ResponseSchema(name=\"reason\",\n",
    "                                    description=\"The reason for the classification\")\n",
    "\n",
    "response_schemas = [level_schema, reason_schema]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)\n",
    "\n",
    "text = \"\"\"Hallo, ich heisse Adam. Ich habe 25 Jahre alt. Ich wohne in England seit 15 Jahren aber ich wuerde gerne irgendwo anders wohnen. Ich liebe es zu reisen. Meiner Meinung nach, ist man wirklich am leben, wenn man reist.\"\"\"\n",
    "\n",
    "# messages = prompt.format(text=text, format_instructions=format_instructions)\n",
    "\n",
    "# messages = [\n",
    "#     {'text': text, 'format_instructions': format_instructions}\n",
    "# ]\n",
    "# response = chain.apply(messages)\n",
    "\n",
    "messages = {'text': text, 'format_instructions': format_instructions}\n",
    "response = chain.run(messages)\n",
    "\n",
    "result = output_parser.parse(response)\n",
    "print(result)\n",
    "type(result)\n",
    "# response[0]['text']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correct Grammar and Spelling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"corrected_text\": string  // The corrected text\n",
      "\t\"reasons\": string  // The reasons for the corrections\n",
      "}\n",
      "```\n",
      "{'corrected_text': 'Hallo, ich heiße Adam. Ich bin 25 Jahre alt. Ich wohne seit 15 Jahren in England, aber ich würde gerne irgendwo anders wohnen. Ich liebe es zu reisen. Meiner Meinung nach ist man wirklich am Leben, wenn man reist.', 'reasons': \"1. Corrected the spelling of 'heisse' to 'heiße' because it should have an 'ß' instead of 'ss'.\\n2. Changed 'habe' to 'bin' because '25 Jahre alt' is a state of being, not possession.\\n3. Added a comma after 'England' to separate the clauses.\\n4. Removed the space before 'wuerde' to correct the spelling.\\n5. Removed the space before 'Meiner' to correct the sentence structure.\\n6. Removed the comma after 'nach' to improve the flow of the sentence.\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": "dict"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "gram_spell_template =\"\"\"Correct the grammar and spelling of the text. Provide a reason for\n",
    "each correction.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "{format_instructions}\n",
    " \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(gram_spell_template)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define output schema\n",
    "corrected_schema = ResponseSchema(name=\"corrected_text\",\n",
    "                                    description=\"The corrected text\")\n",
    "reasons_schema = ResponseSchema(name=\"reasons\",\n",
    "                                description=\"The reasons for the corrections\")\n",
    "\n",
    "\n",
    "response_schemas = [corrected_schema, reasons_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)\n",
    "\n",
    "text = \"\"\"Hallo, ich heisse Adam. Ich habe 25 Jahre alt. Ich wohne in England seit 15 Jahren aber ich wuerde gerne irgendwo anders wohnen. Ich liebe es zu reisen. Meiner Meinung nach, ist man wirklich am leben, wenn man reist.\"\"\"\n",
    "\n",
    "messages = {\n",
    "    'text': text,\n",
    "    'format_instructions': format_instructions\n",
    "}\n",
    "response = chain.run(messages)\n",
    "# response\n",
    "\n",
    "result = output_parser.parse(response)\n",
    "print(result)\n",
    "type(result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Based on the provided text \"Hallo, ich bin 25 Jahre alt,\" it can be classified as A1 level according to the Common European Framework of Reference for Languages (CEFR). \\n\\nThe reason for this classification is that the text consists of simple and basic vocabulary and sentence structures. It includes a greeting (\"Hallo\") and a simple statement about the person\\'s age (\"ich bin 25 Jahre alt\"). A1 level is the beginner level, where learners can understand and use familiar everyday expressions and basic phrases to communicate simple and immediate needs.'}, {'text': 'According to the Common European Framework of Reference for Languages (CEFR), the text \"Wie geht es dir?\" can be classified as A1 level. \\n\\nThe reason for this classification is that the text consists of a simple question in German, asking \"How are you?\" This type of question is commonly taught at the beginner level, where learners are introduced to basic greetings and expressions. At the A1 level, learners are expected to understand and use simple everyday phrases and expressions, as well as be able to interact in a simple way, provided the other person speaks slowly and clearly.'}, {'text': 'Based on the given text, it can be classified as A1 level according to the Common European Framework of Reference for Languages (CEFR). \\n\\nReason: The text consists of simple and basic vocabulary and sentence structures. It talks about a personal preference in a straightforward manner without any complex language usage. This level is characterized by the ability to understand and use familiar everyday expressions and very basic phrases related to personal information and needs.'}]\n",
      "apply time taken: 9.38 seconds\n",
      "\n",
      "[{'text': 'Based on the provided text \"Hallo, ich bin 25 Jahre alt,\" it can be classified as A1 level according to the Common European Framework of Reference for Languages (CEFR). \\n\\nThe reason for this classification is that the text consists of simple and basic vocabulary and sentence structures. It includes a greeting (\"Hallo\") and a simple statement about the person\\'s age (\"ich bin 25 Jahre alt\"). A1 level is the beginner level, where learners can understand and use familiar everyday expressions and basic phrases to communicate simple and immediate needs.'}, {'text': 'According to the Common European Framework of Reference for Languages (CEFR), the text \"Wie geht es dir?\" can be classified as A1 level. \\n\\nThe reason for this classification is that the text is a simple and common question in German, which is typically taught at the beginner level. A1 level learners are expected to be able to understand and use basic phrases and expressions related to personal information, daily routines, and simple interactions. \"Wie geht es dir?\" translates to \"How are you?\" in English, and it is one of the first phrases that beginners learn when studying German.'}, {'text': 'Based on the given text, it can be classified as A1 level according to the Common European Framework of Reference for Languages (CEFR). \\n\\nReason: The text consists of simple and basic vocabulary and sentence structures. It talks about a personal preference in a straightforward manner without any complex language usage. This level is characterized by the ability to understand and use familiar everyday expressions and very basic phrases related to personal information and needs.'}]\n",
      "aapply time taken: 3.56 seconds\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    {'text': 'Hallo, ich bin 25 Jahre alt.'},\n",
    "    {'text': 'Wie geht es dir?'},\n",
    "    {'text': 'In meiner Freizeit spiele ich gerne Fussball.'}\n",
    "]\n",
    "start = time()\n",
    "res_a = chain.apply(texts)\n",
    "print(res_a)\n",
    "print(f\"apply time taken: {time() - start:.2f} seconds\")\n",
    "print()\n",
    "\n",
    "start = time()\n",
    "res_aa = await chain.aapply(texts)\n",
    "print(res_aa)\n",
    "print(f\"aapply time taken: {time() - start:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing some input keys: {'text'}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mchain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtexts\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\chatgpt\\lib\\site-packages\\langchain\\chains\\llm.py:213\u001B[0m, in \u001B[0;36mLLMChain.predict\u001B[1;34m(self, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, callbacks: Callbacks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m    199\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001B[39;00m\n\u001B[0;32m    200\u001B[0m \n\u001B[0;32m    201\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    211\u001B[0m \u001B[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001B[39;00m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_key]\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\chatgpt\\lib\\site-packages\\langchain\\chains\\base.py:123\u001B[0m, in \u001B[0;36mChain.__call__\u001B[1;34m(self, inputs, return_only_outputs, callbacks)\u001B[0m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    108\u001B[0m     inputs: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Any],\n\u001B[0;32m    109\u001B[0m     return_only_outputs: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    110\u001B[0m     callbacks: Callbacks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    111\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[0;32m    112\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Run the logic of this chain and add to output if desired.\u001B[39;00m\n\u001B[0;32m    113\u001B[0m \n\u001B[0;32m    114\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    121\u001B[0m \n\u001B[0;32m    122\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 123\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprep_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    124\u001B[0m     callback_manager \u001B[38;5;241m=\u001B[39m CallbackManager\u001B[38;5;241m.\u001B[39mconfigure(\n\u001B[0;32m    125\u001B[0m         callbacks, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose\n\u001B[0;32m    126\u001B[0m     )\n\u001B[0;32m    127\u001B[0m     new_arg_supported \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\chatgpt\\lib\\site-packages\\langchain\\chains\\base.py:216\u001B[0m, in \u001B[0;36mChain.prep_inputs\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m    214\u001B[0m     external_context \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory\u001B[38;5;241m.\u001B[39mload_memory_variables(inputs)\n\u001B[0;32m    215\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mexternal_context)\n\u001B[1;32m--> 216\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m inputs\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\chatgpt\\lib\\site-packages\\langchain\\chains\\base.py:83\u001B[0m, in \u001B[0;36mChain._validate_inputs\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m     81\u001B[0m missing_keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_keys)\u001B[38;5;241m.\u001B[39mdifference(inputs)\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m missing_keys:\n\u001B[1;32m---> 83\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing some input keys: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmissing_keys\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Missing some input keys: {'text'}"
     ]
    }
   ],
   "source": [
    "chain.predict(a=texts[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LLMChain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product: Queen Size Sheet Set, Generated name: \"Royal Dreams Linens\" or \"Regal Comfort Bedding\"\n",
      "Product: Waterproof Phone Pouch, Generated name: Aquashield\n",
      "Product: Luxury Air Mattress, Generated name: 1. Opulent Rest\n",
      "2. LuxeSlumber\n",
      "3. Supreme Sleep\n",
      "4. Elite Air Mattress Co.\n",
      "5. DreamSerenity\n",
      "6. LuxAir Mattress\n",
      "7. PlushDreams\n",
      "8. PrestigeSleep\n",
      "9. RegalAir Mattress\n",
      "10. SereneSlumber\n",
      "Product: Pillows Insert, Generated name: The Cushion Corner\n",
      "Product: Milk Frother Handheld\n",
      ", Generated name: \"Milk Frother Plus\" or \"FrothMaster\"\n"
     ]
    }
   ],
   "source": [
    "# High temp -> more creative names\n",
    "llm = ChatOpenAI(temperature=0.9)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    'What is the best name to describe a company that sells {product}?',\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "for product in df.Product.iloc[:5]:\n",
    "    company_name_idea = chain.run(product)\n",
    "    print(f\"Product: {product}, Generated name: {company_name_idea}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "LLMResult(generations=[[ChatGeneration(text='RoyalComfort Beddings', generation_info=None, message=AIMessage(content='RoyalComfort Beddings', additional_kwargs={}, example=False))], [ChatGeneration(text='AquaGuard Phone Accessories', generation_info=None, message=AIMessage(content='AquaGuard Phone Accessories', additional_kwargs={}, example=False))], [ChatGeneration(text='OpulentRest', generation_info=None, message=AIMessage(content='OpulentRest', additional_kwargs={}, example=False))], [ChatGeneration(text='PillowPerfect', generation_info=None, message=AIMessage(content='PillowPerfect', additional_kwargs={}, example=False))], [ChatGeneration(text='FrothMaster', generation_info=None, message=AIMessage(content='FrothMaster', additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 116, 'completion_tokens': 19, 'total_tokens': 135}, 'model_name': 'gpt-3.5-turbo'})"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also generate multiple results at once like this\n",
    "products = list(df.Product.iloc[:5])\n",
    "products = [{'product': product} for product in products]\n",
    "chain.generate(products)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: you can use OpenAI via your Azure deployment like so\n",
    "\n",
    "```python\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "deployment_name=\"your-azure-deployment\",\n",
    "model_name=\"text-davinci-003\"\n",
    ")\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SimpleSequentialChain\n",
    "\n",
    "The simplest form of sequential chains, where each step has a singular input/output, and the output of one step is the input to the next."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new SimpleSequentialChain chain...\u001B[0m\n",
      "\u001B[36;1m\u001B[1;3mRoyal Comfort Beddings\u001B[0m\n",
      "\u001B[33;1m\u001B[1;3mRoyal Comfort Beddings is a luxury bedding brand offering top-quality, stylish bedding sets and accessories for a blissful sleep experience.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new SimpleSequentialChain chain...\u001B[0m\n",
      "\u001B[36;1m\u001B[1;3mAquaGuard Phone Protection\u001B[0m\n",
      "\u001B[33;1m\u001B[1;3mAquaGuard Phone Protection offers high-quality waterproof cases and screen protectors to ensure maximum protection for your valuable smartphones.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new SimpleSequentialChain chain...\u001B[0m\n",
      "\u001B[36;1m\u001B[1;3mOpulent Rest\u001B[0m\n",
      "\u001B[33;1m\u001B[1;3mOpulent Rest offers luxurious and high-quality bedding products that provide the ultimate comfort and relaxation for a restful night's sleep.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new SimpleSequentialChain chain...\u001B[0m\n",
      "\u001B[36;1m\u001B[1;3mPillowPro\u001B[0m\n",
      "\u001B[33;1m\u001B[1;3mPillowPro offers premium and customizable pillows designed for optimal comfort and support, ensuring a restful night's sleep.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new SimpleSequentialChain chain...\u001B[0m\n",
      "\u001B[36;1m\u001B[1;3mFrothMaster\u001B[0m\n",
      "\u001B[33;1m\u001B[1;3mFrothMaster is a leading manufacturer of high-quality milk frothers, delivering perfectly frothed milk for your favorite coffee creations.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    'What is the best name to describe a company that sells {product}?',\n",
    ")\n",
    "first_chain = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    'Write a 20 word description for the following company: {company_name}',\n",
    ")\n",
    "second_chain = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[first_chain, second_chain],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "for product in df.Product.iloc[:5]:\n",
    "    overall_simple_chain.run(product)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SequentialChain\n",
    "\n",
    "A more general form of sequential chains, allowing for multiple inputs/outputs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    'Translate the following review to English: {review}',\n",
    ")\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key='english_review')\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    'Summarize the following review in 1 sentence: {english_review}',\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key='summary')\n",
    "\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    'What language is the following review: {review}'\n",
    ")\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key='language')\n",
    "\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    'What a follow up response to the following summary in the specified'\n",
    "    'language: Summary: {summary}\\n\\n Language: {language}'\n",
    ")\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key='follow_up_response')\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=['review'],\n",
    "    output_variables=['english_review', 'summary', 'follow_up_response'],\n",
    "    verbose=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
